{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Subclassing\n",
    "\n",
    "Keras provides a mechanism for subclassing `tf.keras.Model`, allowing you to \n",
    "elegantly define your own models that can be used with any high level API\n",
    "call that expects a `Model` object. The code below shows the construction of\n",
    "a residual style vision network, modeled after Resnet50.\n",
    "\n",
    "The process of model subclassing follows a pattern:\n",
    "    1. Define a `class` that inherits from `tf.keras.Model`\n",
    "    2. Define a `__init__` method that defines the layers to be used in the model\n",
    "    3. Define a `__call__` method that chains the defined layers together \n",
    "    into a flow of information\n",
    "\n",
    "Note that subclassed models can include other models as part of the computational\n",
    "flow, allowing for great reuse of fundamental building blocks.\n",
    "\n",
    "## Tail\n",
    "\n",
    "As an example, we will begin by constructing a tail consisting of a $7 \\times 7 / 2$\n",
    "convolution and a $3 \\times 3 / 2$ max pool. The `__init__` function defines the layers,\n",
    "and the `__call__` function describes the flow of data through these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Tail(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, Ni, *args, **kwargs):\n",
    "\n",
    "        # Call parent constructor with *args, **kwargs\n",
    "        super(Tail, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = layers.Conv2D(\n",
    "                filters=Ni,\n",
    "                kernel_size=(7, 7),\n",
    "                strides=2,\n",
    "                use_bias=False,\n",
    "                name='tail_conv')\n",
    "\n",
    "        # Batch norm\n",
    "        self.bn = layers.BatchNormalization(\n",
    "                name='tail_bn')\n",
    "\n",
    "        # ReLU\n",
    "        self.relu = layers.ReLU(name='tail_relu')\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = layers.MaxPool2D(\n",
    "                pool_size=(2, 2),\n",
    "                strides=2,\n",
    "                name='tail_pool')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Residual forward pass\n",
    "        _ = self.conv(inputs, **kwargs)\n",
    "\n",
    "        # Must call with **kwargs to receive training state\n",
    "        _ = self.bn(_, **kwargs)\n",
    "\n",
    "        _ = self.relu(_, **kwargs)\n",
    "\n",
    "        return self.pool(_, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Block\n",
    "\n",
    "Next we define the fundamental CNN style 2D convolution block\n",
    "of Resnet, ie batch-norm, relu, convolution.\n",
    "\n",
    "Note that the number of filters and the kernel size are\n",
    "parameterized, and that parameter packs `*args, **kwargs`\n",
    "are forwarded to the convolution layer. This is important\n",
    "as it enables the reuse of this model for the various\n",
    "types of convolutions that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBasic(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, filters, kernel_size, strides=(1,1), *args, **kwargs):\n",
    "        super(ResnetBasic, self).__init__(*args, **kwargs)\n",
    "        self.batch_norm = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "        self.conv2d = layers.Conv2D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding='same',\n",
    "                activation=None,\n",
    "                use_bias=False,\n",
    "                strides=strides)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.batch_norm(inputs, **kwargs)\n",
    "        x = self.relu(x, **kwargs)\n",
    "        return self.conv2d(x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Bottleneck\n",
    "\n",
    "We can use `ResnetBasic` to build a bottleneck layer. Again we leave the\n",
    "number of input feature maps parameterized so that way may reuse the\n",
    "`Bottleneck` model at each level of downsampling.\n",
    "\n",
    "Note that here we use loops in the `__init__` function. This helps define\n",
    "repeating structures, but recall that in the `__init__` function we are only\n",
    "defining layers and not the flow of computation between them. The use of a loop\n",
    "in `__call__` is where we define the sequential flow through each iteration in\n",
    "the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, Ni, *args, **kwargs):\n",
    "        super(Bottleneck, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Three residual convolution blocks\n",
    "        kernels = [(1, 1), (3, 3), (1, 1)]\n",
    "        feature_maps = [Ni // 4, Ni // 4, Ni]\n",
    "        self.residual_filters = [\n",
    "            ResnetBasic(N, K)\n",
    "            for N, K in zip(feature_maps, kernels)\n",
    "        ]\n",
    "\n",
    "        # Merge operation\n",
    "        self.merge = layers.Add()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        # Residual forward pass\n",
    "        res = inputs\n",
    "        for res_layer in self.residual_filters:\n",
    "            res = res_layer(res, **kwargs)\n",
    "\n",
    "        # Combine residual pass with identity\n",
    "        return self.merge([inputs, res], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Bottleneck\n",
    "\n",
    "We can define the special bottleneck layer by subclassing\n",
    "the `Bottleneck` class. We add a convolutional layer along\n",
    "the main path and redefine the `__call__` method to include\n",
    "this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialBottleneck(Bottleneck):\n",
    "\n",
    "    def __init__(self, Ni, *args, **kwargs):\n",
    "\n",
    "        # Layers that also appear in standard bottleneck\n",
    "        super(SpecialBottleneck, self).__init__(Ni, *args, **kwargs)\n",
    "\n",
    "        # Add convolution layer along main path\n",
    "        self.main = layers.Conv2D(\n",
    "                Ni,\n",
    "                (1, 1),\n",
    "                padding='same',\n",
    "                \n",
    "                activation=None,\n",
    "                use_bias=False)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        # Residual forward pass\n",
    "        res = inputs\n",
    "        for res_layer in self.residual_filters:\n",
    "            res = res_layer(res, **kwargs)\n",
    "\n",
    "        # Convolution on main forward pass\n",
    "        main = self.main(inputs, **kwargs)\n",
    "\n",
    "        # Merge residual and main\n",
    "        return self.merge([main, res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "Next we need to define the downsampling layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, Ni, *args, **kwargs):\n",
    "        super(Downsample, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Three residual convolution blocks\n",
    "        kernels = [(1, 1), (3, 3), (1, 1)]\n",
    "        strides = [(2, 2), (1, 1), (1, 1)]\n",
    "        feature_maps = [Ni // 2, Ni // 2, 2*Ni]\n",
    "\n",
    "        self.residual_filters = [\n",
    "            ResnetBasic(N, K, strides=S)\n",
    "            for N, K, S in zip(feature_maps, kernels, strides)\n",
    "        ]\n",
    "\n",
    "        # Convolution on main path\n",
    "        self.main = ResnetBasic(2*Ni, (1,1), strides=(2,2))\n",
    "\n",
    "        # Merge operation for residual and main\n",
    "        self.merge = layers.Add()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        # Residual forward pass\n",
    "        res = inputs\n",
    "        for res_layer in self.residual_filters:\n",
    "            res = res_layer(res,**kwargs)\n",
    "\n",
    "        # Main forward pass\n",
    "        main = self.main(inputs, **kwargs)\n",
    "\n",
    "        # Merge residual and main\n",
    "        return self.merge([main, res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "Finally, we can assemble these blocks into the final model. Note that\n",
    "a levels argument is added to the constructor to parameterize the number\n",
    "of bottleneck repeats at each level of downsampling. The `levels` parameter\n",
    "receives a list of integers where `levels[i]` gives the number of bottleneck\n",
    "repeats at level `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, classes, filters, levels, *args, **kwargs):\n",
    "        super(Resnet, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "        # Lists to hold various layers\n",
    "        self.blocks = list()\n",
    "\n",
    "        # Tail\n",
    "        self.tail = Tail(filters)\n",
    "\n",
    "        # Special bottleneck layer with convolution on main path\n",
    "        self.level_0_special = SpecialBottleneck(filters)\n",
    "\n",
    "        # Loop through levels and their parameterized repeat counts\n",
    "        for level, repeats in enumerate(levels):\n",
    "            for block in range(repeats):\n",
    "                # Append a bottleneck block for each repeat\n",
    "                name = 'bottleneck_%i_%i' % (level, block)\n",
    "                layer = Bottleneck(filters, name=name)\n",
    "                self.blocks.append(layer)\n",
    "\n",
    "            # Downsample and double feature maps at end of level\n",
    "            name = 'downsample_%i' % (level)\n",
    "            layer = Downsample(filters, name=name)\n",
    "            self.blocks.append(layer)\n",
    "            filters *= 2\n",
    "\n",
    "        self.level2_batch_norm = layers.BatchNormalization(name='final_bn')\n",
    "        self.level2_relu = layers.ReLU(name='final_relu')\n",
    "\n",
    "        # Decoder - global average pool and fully connected\n",
    "        self.global_avg = layers.GlobalAveragePooling2D(\n",
    "                name='GAP'\n",
    "        )\n",
    "\n",
    "        # Dense with regularizer, just as a test\n",
    "        self.dense = layers.Dense(\n",
    "                classes,\n",
    "                name='dense',\n",
    "                # Just for fun, show a regularized layer\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                use_bias=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.tail(inputs, **kwargs)\n",
    "        x = self.level_0_special(x)\n",
    "\n",
    "        # Loop over layers by level\n",
    "        for layer in self.blocks:\n",
    "            x = layer(x, **kwargs)\n",
    "\n",
    "        # Finish up specials in level 2\n",
    "        x = self.level2_batch_norm(x, **kwargs)\n",
    "        x = self.level2_relu(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.global_avg(x)\n",
    "        return self.dense(x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "Now we can construct the model. Here we define four level model\n",
    "with bottleneck repeats given in `levels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "levels = [4, 3, 6, 2]\n",
    "num_classes = 100\n",
    "width = 32\n",
    "model = Resnet(num_classes, width, levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now we have constructed a model, but the model still knows nothing\n",
    "about the actual input sizes it will be working with. We can define\n",
    "an input layer and call the model on this input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.InputLayer in TF 2.0\n",
    "inputs = tf.keras.layers.InputLayer(\n",
    "        input_shape=(64, 64, 3),\n",
    "        batch_size=32,\n",
    "        dtype=tf.float32\n",
    ")\n",
    "outputs = model(inputs.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get information about the model, ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bottleneck_0_0 (Bottleneck)  (32, 14, 14, 32)          1280      \n",
      "_________________________________________________________________\n",
      "bottleneck_0_1 (Bottleneck)  (32, 14, 14, 32)          1280      \n",
      "_________________________________________________________________\n",
      "bottleneck_0_2 (Bottleneck)  (32, 14, 14, 32)          1280      \n",
      "_________________________________________________________________\n",
      "bottleneck_0_3 (Bottleneck)  (32, 14, 14, 32)          1280      \n",
      "_________________________________________________________________\n",
      "downsample_0 (Downsample)    (32, 7, 7, 64)            6272      \n",
      "_________________________________________________________________\n",
      "bottleneck_1_0 (Bottleneck)  (32, 7, 7, 64)            4736      \n",
      "_________________________________________________________________\n",
      "bottleneck_1_1 (Bottleneck)  (32, 7, 7, 64)            4736      \n",
      "_________________________________________________________________\n",
      "bottleneck_1_2 (Bottleneck)  (32, 7, 7, 64)            4736      \n",
      "_________________________________________________________________\n",
      "downsample_1 (Downsample)    (32, 4, 4, 128)           24320     \n",
      "_________________________________________________________________\n",
      "bottleneck_2_0 (Bottleneck)  (32, 4, 4, 128)           18176     \n",
      "_________________________________________________________________\n",
      "bottleneck_2_1 (Bottleneck)  (32, 4, 4, 128)           18176     \n",
      "_________________________________________________________________\n",
      "bottleneck_2_2 (Bottleneck)  (32, 4, 4, 128)           18176     \n",
      "_________________________________________________________________\n",
      "bottleneck_2_3 (Bottleneck)  (32, 4, 4, 128)           18176     \n",
      "_________________________________________________________________\n",
      "bottleneck_2_4 (Bottleneck)  (32, 4, 4, 128)           18176     \n",
      "_________________________________________________________________\n",
      "bottleneck_2_5 (Bottleneck)  (32, 4, 4, 128)           18176     \n",
      "_________________________________________________________________\n",
      "downsample_2 (Downsample)    (32, 2, 2, 256)           95744     \n",
      "_________________________________________________________________\n",
      "bottleneck_3_0 (Bottleneck)  (32, 2, 2, 256)           71168     \n",
      "_________________________________________________________________\n",
      "bottleneck_3_1 (Bottleneck)  (32, 2, 2, 256)           71168     \n",
      "_________________________________________________________________\n",
      "downsample_3 (Downsample)    (32, 1, 1, 512)           379904    \n",
      "_________________________________________________________________\n",
      "tail (Tail)                  (32, 14, 14, 32)          4832      \n",
      "_________________________________________________________________\n",
      "special_bottleneck (SpecialB (32, 14, 14, 32)          2304      \n",
      "_________________________________________________________________\n",
      "final_bn (BatchNormalization (32, 1, 1, 512)           2048      \n",
      "_________________________________________________________________\n",
      "final_relu (ReLU)            (32, 1, 1, 512)           0         \n",
      "_________________________________________________________________\n",
      "GAP (GlobalAveragePooling2D) (32, 512)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, 100)                 51300     \n",
      "=================================================================\n",
      "Total params: 837,444\n",
      "Trainable params: 828,580\n",
      "Non-trainable params: 8,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can iterate over layers and examine properties of\n",
    "the individual `Layer` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck             : (32, 14, 14, 32) -> (32, 14, 14, 32)\n",
      "Bottleneck             : (32, 14, 14, 32) -> (32, 14, 14, 32)\n",
      "Bottleneck             : (32, 14, 14, 32) -> (32, 14, 14, 32)\n",
      "Bottleneck             : (32, 14, 14, 32) -> (32, 14, 14, 32)\n",
      "Downsample             : (32, 14, 14, 32) -> (32, 7, 7, 64) \n",
      "Bottleneck             :  (32, 7, 7, 64) -> (32, 7, 7, 64) \n",
      "Bottleneck             :  (32, 7, 7, 64) -> (32, 7, 7, 64) \n",
      "Bottleneck             :  (32, 7, 7, 64) -> (32, 7, 7, 64) \n",
      "Downsample             :  (32, 7, 7, 64) -> (32, 4, 4, 128)\n",
      "Bottleneck             : (32, 4, 4, 128) -> (32, 4, 4, 128)\n",
      "Bottleneck             : (32, 4, 4, 128) -> (32, 4, 4, 128)\n",
      "Bottleneck             : (32, 4, 4, 128) -> (32, 4, 4, 128)\n",
      "Bottleneck             : (32, 4, 4, 128) -> (32, 4, 4, 128)\n",
      "Bottleneck             : (32, 4, 4, 128) -> (32, 4, 4, 128)\n",
      "Bottleneck             : (32, 4, 4, 128) -> (32, 4, 4, 128)\n",
      "Downsample             : (32, 4, 4, 128) -> (32, 2, 2, 256)\n",
      "Bottleneck             : (32, 2, 2, 256) -> (32, 2, 2, 256)\n",
      "Bottleneck             : (32, 2, 2, 256) -> (32, 2, 2, 256)\n",
      "Downsample             : (32, 2, 2, 256) -> (32, 1, 1, 512)\n",
      "Tail                   : (32, 64, 64, 3) -> (32, 14, 14, 32)\n",
      "SpecialBottleneck      : (32, 14, 14, 32) -> (32, 14, 14, 32)\n",
      "BatchNormalizationV1   : (32, 1, 1, 512) -> (32, 1, 1, 512)\n",
      "ReLU                   : (32, 1, 1, 512) -> (32, 1, 1, 512)\n",
      "GlobalAveragePooling2D : (32, 1, 1, 512) -> (32, 512)      \n",
      "Dense                  :       (32, 512) -> (32, 100)      \n"
     ]
    }
   ],
   "source": [
    "FMT = \"%-22s : %15s -> %-15s\"\n",
    "for layer in model.layers:\n",
    "    name = type(layer).__name__\n",
    "    inp= layer.input_shape \n",
    "    out= layer.output_shape\n",
    "    msg = FMT % (name, inp, out)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An upcoming demo will show how to train the constructed model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
